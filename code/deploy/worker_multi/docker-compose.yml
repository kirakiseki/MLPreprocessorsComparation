name: bigdata_worker

services:
  spark-worker-1:
    image: spark:4.0.1-scala2.13-java21-python3-ubuntu
    user: root
    container_name: bigdata_spark_worker_1
    hostname: spark-worker-1
    restart: unless-stopped
    networks:
      - bigdata
    ports:
      - "8081:8081"
      - "7078:7078"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_PORT=7078
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_OPTS=-Dspark.worker.idPattern=worker-1
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_URL=${MINIO_URL}
    volumes:
      - ./assets/spark/conf/metrics.properties:/opt/spark/conf/metrics.properties
      - ./assets/spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./assets/spark/sbin/start-daemon.sh:/opt/spark/sbin/spark-daemon.sh
      - ./assets/spark/jars:/ext_jars:ro
      - ./data/spark_1/logs:/opt/spark/logs
    command: >
      bash -c "rm -rf /opt/spark/logs/* && cp /ext_jars/* /opt/spark/jars/ && chmod a+x /opt/spark/sbin/spark-daemon.sh && /opt/spark/sbin/start-worker.sh --properties-file /opt/spark/conf/metrics.properties --properties-file /opt/spark/conf/spark-defaults.conf spark://spark-master:7077 && tail -f /dev/null"
    shm_size: '16gb'
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G

  ray-worker-1:
    image: rayproject/ray:2.51.1-py311-cpu
    user: root
    container_name: bigdata_ray_worker_1
    hostname: ray-worker-1
    restart: unless-stopped
    networks:
      - bigdata
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_URL=${MINIO_URL}
    volumes:
      - ./data/ray_1/tmp:/ray_tmp
    ports:
      - "8266:8265"
    command: >
      ray start --address='ray-head:6379' --dashboard-host 0.0.0.0 --node-ip-address=ray-worker-1 --block
    shm_size: '16gb'
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G

  spark-worker-2:
    image: spark:4.0.1-scala2.13-java21-python3-ubuntu
    user: root
    container_name: bigdata_spark_worker_2
    hostname: spark-worker-2
    restart: unless-stopped
    networks:
      - bigdata
    ports:
      - "8082:8082"
      - "7079:7079"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_PORT=7079
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_WORKER_OPTS=-Dspark.worker.idPattern=worker-2
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_URL=${MINIO_URL}
    volumes:
      - ./assets/spark/conf/metrics.properties:/opt/spark/conf/metrics.properties
      - ./assets/spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./assets/spark/sbin/start-daemon.sh:/opt/spark/sbin/spark-daemon.sh
      - ./assets/spark/jars:/ext_jars:ro
      - ./data/spark_2/logs:/opt/spark/logs
    command: >
      bash -c "rm -rf /opt/spark/logs/* && cp /ext_jars/* /opt/spark/jars/ && chmod a+x /opt/spark/sbin/spark-daemon.sh && /opt/spark/sbin/start-worker.sh --properties-file /opt/spark/conf/metrics.properties --properties-file /opt/spark/conf/spark-defaults.conf spark://spark-master:7077 && tail -f /dev/null"
    shm_size: '16gb'
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G

  ray-worker-2:
    image: rayproject/ray:2.51.1-py311-cpu
    user: root
    container_name: bigdata_ray_worker_2
    hostname: ray-worker-2
    restart: unless-stopped
    networks:
      - bigdata
    environment:      
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_URL=${MINIO_URL}
    volumes:
      - ./data/ray_2/tmp:/ray_tmp
    ports:
      - "8267:8265"
    command: >
      ray start --address='ray-head:6379' --dashboard-host 0.0.0.0 --node-ip-address=ray-worker-2 --block
    shm_size: '16gb'
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G

  spark-worker-3:
    image: spark:4.0.1-scala2.13-java21-python3-ubuntu
    user: root
    container_name: bigdata_spark_worker_3
    hostname: spark-worker-3
    restart: unless-stopped
    networks:
      - bigdata
    ports:
      - "8083:8083"
      - "7080:7080"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_PORT=7080
      - SPARK_WORKER_WEBUI_PORT=8083
      - SPARK_WORKER_OPTS=-Dspark.worker.idPattern=worker-3
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_URL=${MINIO_URL}
    volumes:
      - ./assets/spark/conf/metrics.properties:/opt/spark/conf/metrics.properties
      - ./assets/spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./assets/spark/sbin/start-daemon.sh:/opt/spark/sbin/spark-daemon.sh
      - ./assets/spark/jars:/ext_jars:ro
      - ./data/spark_3/logs:/opt/spark/logs
    command: >
      bash -c "rm -rf /opt/spark/logs/* && cp /ext_jars/* /opt/spark/jars/ && chmod a+x /opt/spark/sbin/spark-daemon.sh && /opt/spark/sbin/start-worker.sh --properties-file /opt/spark/conf/metrics.properties --properties-file /opt/spark/conf/spark-defaults.conf spark://spark-master:7077 && tail -f /dev/null"
    shm_size: '16gb'
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G

  ray-worker-3:
    image: rayproject/ray:2.51.1-py311-cpu
    user: root
    container_name: bigdata_ray_worker_3
    hostname: ray-worker-3
    restart: unless-stopped
    networks:
      - bigdata
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_URL=${MINIO_URL}
    volumes:
      - ./data/ray_3/tmp:/ray_tmp
    ports:
      - "8268:8265"
    command: >
      ray start --address='ray-head:6379' --dashboard-host 0.0.0.0 --node-ip-address=ray-worker-3 --block
    shm_size: '16gb'
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G

networks:
  bigdata:
    external: true